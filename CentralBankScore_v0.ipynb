{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Central Bank Score\n",
        "\n",
        "## An√°lise de Sentimentos em Discursos de Bancos Centrais\n",
        "\n",
        "Este projeto realiza an√°lise de sentimentos em discursos de Bancos Centrais de diversos pa√≠ses,\n",
        "gerando um **score Hawkish-Dovish** para simplificar a interpreta√ß√£o econ√¥mica dos comunicados.\n",
        "\n",
        "### Objetivos\n",
        "- Identificar t√≥picos principais nos discursos usando **LDA**\n",
        "- Analisar sentimentos com **VADER**\n",
        "- Calcular √≠ndice de tom (Hawkish vs Dovish)\n",
        "- Visualizar tend√™ncias por pa√≠s e per√≠odo\n",
        "\n",
        "###  Dataset\n",
        "- **Fonte**: Kaggle - Central Bank Speeches\n",
        "- **Tamanho**: ~7.700 discursos em ingl√™s\n",
        "- **Per√≠odo**: V√°rios anos de discursos de Bancos Centrais globais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Importa√ß√µes necess√°rias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Configurar estilo dos gr√°ficos\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print('‚úÖ Bibliotecas importadas com sucesso!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Download dos recursos NLTK necess√°rios\n",
        "nltk_resources = [\n",
        "    'stopwords',\n",
        "    'punkt',\n",
        "    'punkt_tab',  \n",
        "    'wordnet',\n",
        "    'vader_lexicon',\n",
        "    'averaged_perceptron_tagger',  \n",
        "    'omw-eng'  # Open Multilingual WordNet \n",
        "]\n",
        "\n",
        "print('üì• Baixando recursos do NLTK...')\n",
        "print('=' * 40)\n",
        "\n",
        "for resource in nltk_resources:\n",
        "    try:\n",
        "        nltk.download(resource, quiet=False)  # quiet=False para ver progresso\n",
        "        print(f'‚úÖ {resource} baixado com sucesso!')\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è Erro ao baixar {resource}: {e}')\n",
        "\n",
        "print('=' * 40)\n",
        "print('‚úÖ Recursos NLTK baixados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Carregar a base de dados\n",
        "file_path = 'all_speeches.csv'\n",
        "\n",
        "# Verificar se o arquivo existe\n",
        "if os.path.exists(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f'‚úÖ Dataset carregado com sucesso!')\n",
        "    print(f'üìä Dimens√µes: {df.shape[0]} linhas x {df.shape[1]} colunas')\n",
        "else:\n",
        "    print(f'‚ùå Arquivo n√£o encontrado: {file_path}')\n",
        "    print('üí° Certifique-se de que o arquivo all_speeches.csv est√° na pasta correta')\n",
        "\n",
        "# Converter a coluna de data para datetime\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "# Criar colunas adicionais para an√°lise temporal\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['year_month'] = df['date'].dt.to_period('M').astype(str)\n",
        "\n",
        "# Visualizar primeiras linhas\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ÑπÔ∏è Informa√ß√µes sobre o dataset\n",
        "print('üìã Informa√ß√µes do Dataset:')\n",
        "print('=' * 50)\n",
        "df.info()\n",
        "\n",
        "print('\\nüìà Estat√≠sticas por Pa√≠s:')\n",
        "print('=' * 50)\n",
        "print(df['country'].value_counts())\n",
        "\n",
        "print('\\nüìÖ Per√≠odo dos Dados:')\n",
        "print('=' * 50)\n",
        "print(f'Data inicial: {df[\"date\"].min()}')\n",
        "print(f'Data final: {df[\"date\"].max()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pr√©-processamento dos Textos\n",
        "\n",
        "Nesta etapa, realizamos:\n",
        "1. Remo√ß√£o de pontua√ß√µes\n",
        "2. Remo√ß√£o de stopwords\n",
        "3. Lemmatiza√ß√£o (redu√ß√£o √† raiz das palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fun√ß√£o para remover pontua√ß√£o\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', str(text))\n",
        "\n",
        "# Aplicar limpeza\n",
        "corpus = df['text']\n",
        "clean_corpus = corpus.apply(lambda x: remove_punctuation(x))\n",
        "\n",
        "# Classe para Lemmatiza√ß√£o\n",
        "class Lemmatizer:\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    \n",
        "    def __call__(self, text):\n",
        "        return [self.wnl.lemmatize(word) for word in word_tokenize(text)]\n",
        "\n",
        "# Carregar e processar stop words\n",
        "lemmatizer = Lemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "processed_stop_words = set()\n",
        "\n",
        "for word in stop_words:\n",
        "    processed_stop_words.update(lemmatizer(word))\n",
        "\n",
        "print(f'‚úÖ Pr√©-processamento conclu√≠do!')\n",
        "print(f'üìä Stopwords processadas: {len(processed_stop_words)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¢ Vetoriza√ß√£o do texto\n",
        "MAX_VOCAB_SIZE = 40000\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    tokenizer=Lemmatizer(),\n",
        "    stop_words=list(processed_stop_words),\n",
        "    max_features=MAX_VOCAB_SIZE\n",
        ")\n",
        "\n",
        "data = vectorizer.fit_transform(clean_corpus)\n",
        "\n",
        "print(f'‚úÖ Vetoriza√ß√£o conclu√≠da!')\n",
        "print(f'üìä Shape da matriz: {data.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelagem de T√≥picos com LDA\n",
        "\n",
        "O **Latent Dirichlet Allocation (LDA)** √© um modelo n√£o-supervisionado que identifica\n",
        "t√≥picos ocultos em uma cole√ß√£o de documentos.\n",
        "\n",
        "Identificando **5 t√≥picos principais** nos discursos dos Bancos Centrais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplicar LDA\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=5,\n",
        "    random_state=1234,\n",
        "    max_iter=10\n",
        ")\n",
        "\n",
        "lda.fit(data)\n",
        "\n",
        "print('‚úÖ Modelo LDA treinado!')\n",
        "print('\\nüìã Palavras mais frequentes por t√≥pico:')\n",
        "print('=' * 60)\n",
        "\n",
        "for idx, topic in enumerate(lda.components_):\n",
        "    top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-20 - 1:-1]]\n",
        "    print(f'\\nüìù T√≥pico {idx}:')\n",
        "    print(', '.join(top_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Visualiza√ß√£o com WordCloud\n",
        "def plot_wordcloud(model, feature_names, n_words=20):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        words = [feature_names[i] for i in topic.argsort()[:-n_words - 1:-1]]\n",
        "        wordcloud = WordCloud(\n",
        "            width=800,\n",
        "            height=400,\n",
        "            background_color='white',\n",
        "            colormap='viridis'\n",
        "        ).generate(' '.join(words))\n",
        "        \n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'WordCloud - T√≥pico {topic_idx}', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Plotar WordClouds\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "plot_wordcloud(lda, feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An√°lise de Sentimentos com VADER\n",
        "\n",
        "O **VADER (Valence Aware Dictionary and sEntiment Reasoner)** √© uma ferramenta\n",
        "de an√°lise de sentimentos baseada em l√©xico e regras, especialmente projetada\n",
        "para an√°lise de sentimentos em m√≠dias sociais.\n",
        "\n",
        "M√©tricas:\n",
        "- **Compound**: Score geral de sentimento (-1 a +1)\n",
        "- **Positivo**: Propor√ß√£o de conte√∫do positivo\n",
        "- **Negativo**: Propor√ß√£o de conte√∫do negativo\n",
        "- **Neutro**: Propor√ß√£o de conte√∫do neutro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializar analisador VADER\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Fun√ß√£o para calcular sentimento\n",
        "def sentiment_analysis(text):\n",
        "    return sid.polarity_scores(text)\n",
        "\n",
        "# Aplicar an√°lise de sentimentos\n",
        "print('‚è≥ Aplicando an√°lise de sentimentos (isso pode levar alguns minutos)...')\n",
        "df['sentiment_scores'] = clean_corpus.apply(lambda x: sentiment_analysis(x))\n",
        "\n",
        "# Extrair pontua√ß√µes individuais\n",
        "df['compound'] = df['sentiment_scores'].apply(lambda x: x['compound'])\n",
        "df['positive'] = df['sentiment_scores'].apply(lambda x: x['pos'])\n",
        "df['negative'] = df['sentiment_scores'].apply(lambda x: x['neg'])\n",
        "df['neutral'] = df['sentiment_scores'].apply(lambda x: x['neu'])\n",
        "\n",
        "print('‚úÖ An√°lise de sentimentos conclu√≠da!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Estat√≠sticas de Sentimentos\n",
        "print('üìà M√©dias dos Sentimentos:')\n",
        "print('=' * 40)\n",
        "print(f'Compound Score: {df[\"compound\"].mean():.4f}')\n",
        "print(f'Positivo: {df[\"positive\"].mean():.4f} ({df[\"positive\"].mean()*100:.1f}%)')\n",
        "print(f'Negativo: {df[\"negative\"].mean():.4f} ({df[\"negative\"].mean()*100:.1f}%)')\n",
        "print(f'Neutro: {df[\"neutral\"].mean():.4f} ({df[\"neutral\"].mean()*100:.1f}%)')\n",
        "\n",
        "# Visualizar distribui√ß√£o\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Compound\n",
        "sns.histplot(df['compound'], bins=30, kde=True, ax=axes[0, 0], color='purple')\n",
        "axes[0, 0].set_title('Distribui√ß√£o - Compound Score')\n",
        "axes[0, 0].set_xlabel('Score')\n",
        "\n",
        "# Positivo\n",
        "sns.histplot(df['positive'], bins=30, kde=True, ax=axes[0, 1], color='green')\n",
        "axes[0, 1].set_title('Distribui√ß√£o - Sentimento Positivo')\n",
        "axes[0, 1].set_xlabel('Propor√ß√£o')\n",
        "\n",
        "# Negativo\n",
        "sns.histplot(df['negative'], bins=30, kde=True, ax=axes[1, 0], color='red')\n",
        "axes[1, 0].set_title('Distribui√ß√£o - Sentimento Negativo')\n",
        "axes[1, 0].set_xlabel('Propor√ß√£o')\n",
        "\n",
        "# Neutro\n",
        "sns.histplot(df['neutral'], bins=30, kde=True, ax=axes[1, 1], color='gray')\n",
        "axes[1, 1].set_title('Distribui√ß√£o - Sentimento Neutro')\n",
        "axes[1, 1].set_xlabel('Propor√ß√£o')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ Identificar t√≥pico dominante para cada documento\n",
        "topic_assignments = lda.transform(data)\n",
        "df['dominant_topic'] = topic_assignments.argmax(axis=1)\n",
        "\n",
        "# Calcular m√©dias de sentimentos por t√≥pico\n",
        "topic_sentiment = df.groupby('dominant_topic').agg({\n",
        "    'compound': 'mean',\n",
        "    'positive': 'mean',\n",
        "    'negative': 'mean',\n",
        "    'neutral': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Visualizar\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "metrics = [('compound', 'Compound Score'), ('positive', 'Positivo'),\n",
        "           ('negative', 'Negativo'), ('neutral', 'Neutro')]\n",
        "\n",
        "for idx, (col, title) in enumerate(metrics):\n",
        "    row, col_idx = idx // 2, idx % 2\n",
        "    sns.barplot(x='dominant_topic', y=col, data=topic_sentiment,\n",
        "                palette='viridis', ax=axes[row, col_idx])\n",
        "    axes[row, col_idx].set_title(f'M√©dia - {title} por T√≥pico')\n",
        "    axes[row, col_idx].set_xlabel('T√≥pico Dominante')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nüìä Tabela - M√©dias por T√≥pico:')\n",
        "print(topic_sentiment.round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü¶ÖüïäÔ∏è Score Hawkish-Dovish\n",
        "\n",
        "O score Hawkish-Dovish mede o tom dos discursos dos Bancos Centrais:\n",
        "\n",
        "- **ü¶Ö Hawkish (+1)**: Banco Central comprometido com metas, decis√µes previs√≠veis\n",
        "  - Palavras: high, strong, increase, fast, accelerate, boom, expansion...\n",
        "\n",
        "- **üïäÔ∏è Dovish (-1)**: Banco Central discricion√°rio, decis√µes imprevis√≠veis\n",
        "  - Palavras: low, weak, decrease, slow, recession, decline, contraction...\n",
        "\n",
        "**F√≥rmula:**\n",
        "\\[\\text{tone}_{i,t} = \\frac{\\text{hawkish}_{i,t} - \\text{dovish}_{i,t}}{\\text{hawkish}_{i,t} + \\text{dovish}_{i,t}}\\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir listas de palavras-chave\n",
        "hawkish_keywords = [\n",
        "    'high', 'strong', 'increase', 'fast', 'accelerate', 'better', 'boom', 'emerg', 'expansion',\n",
        "    'favo(u)rabl', 'firm', 'great', 'improv', 'increas', 'larger', 'positive', 'rais', 'risk',\n",
        "    'stabiliz', 'stable', 'strengthen', 'strong', 'subdued', 'unsustainable', 'upside',\n",
        "    'upswing', 'upturn', 'upward'\n",
        "]\n",
        "\n",
        "dovish_keywords = [\n",
        "    'low', 'weak', 'decreas', 'slow', 'collapse', 'contraction', 'dampen', 'decelerate',\n",
        "    'decline', 'decreas', 'delay', 'depression', 'destabiliz', 'deteriorat', 'difficult',\n",
        "    'diminish', 'disappear', 'downside', 'downturn', 'downward', 'fall', 'fragile', 'low',\n",
        "    'negative', 'poor', 'recession', 'slow', 'sluggish', 'small', 'struggling', 'unstable',\n",
        "    'unfavo(u)rabl', 'wors'\n",
        "]\n",
        "\n",
        "# Fun√ß√£o para contar palavras-chave\n",
        "def count_keywords(text, keywords):\n",
        "    count = 0\n",
        "    for keyword in keywords:\n",
        "        count += len(re.findall(keyword, text, re.IGNORECASE))\n",
        "    return count\n",
        "\n",
        "# Aplicar contagem\n",
        "df['hawkish_count'] = clean_corpus.apply(lambda x: count_keywords(x, hawkish_keywords))\n",
        "df['dovish_count'] = clean_corpus.apply(lambda x: count_keywords(x, dovish_keywords))\n",
        "\n",
        "# Calcular √≠ndice de tom\n",
        "def calculate_tone(hawkish_count, dovish_count):\n",
        "    if hawkish_count + dovish_count == 0:\n",
        "        return np.nan\n",
        "    return (hawkish_count - dovish_count) / (hawkish_count + dovish_count)\n",
        "\n",
        "df['tone'] = df.apply(\n",
        "    lambda row: calculate_tone(row['hawkish_count'], row['dovish_count']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print('‚úÖ Score Hawkish-Dovish calculado!')\n",
        "print(f'üìä M√©dia geral do √≠ndice de tom: {df[\"tone\"].mean():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üåç An√°lise do √≠ndice de tom por pa√≠s\n",
        "tone_by_country = df.groupby('country')['tone'].agg(['mean', 'count']).reset_index()\n",
        "tone_by_country.columns = ['country', 'mean_tone', 'count']\n",
        "tone_by_country = tone_by_country.sort_values('mean_tone', ascending=False)\n",
        "\n",
        "print('üìä M√©dia do √çndice de Tom por Pa√≠s:')\n",
        "print('=' * 50)\n",
        "for _, row in tone_by_country.iterrows():\n",
        "    print(f\"{row['country']:<20} {row['mean_tone']:>8.4f}  ({row['count']:>4} discursos)\")\n",
        "\n",
        "# Visualiza√ß√£o\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='country', y='mean_tone', data=tone_by_country, palette='RdYlGn')\n",
        "plt.title('ü¶ÖüïäÔ∏è M√©dia do √çndice de Tom por Pa√≠s', fontsize=14)\n",
        "plt.xlabel('Pa√≠s')\n",
        "plt.ylabel('√çndice de Tom M√©dio')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Distribui√ß√£o do √≠ndice de tom por pa√≠s\n",
        "g = sns.FacetGrid(df, col='country', col_wrap=4, height=3, aspect=1.2)\n",
        "g.map(sns.histplot, 'tone', bins=30, kde=True, color='steelblue')\n",
        "g.set_titles('{col_name}')\n",
        "g.set_axis_labels('√çndice de Tom', 'Frequ√™ncia')\n",
        "g.fig.suptitle('Distribui√ß√£o do √çndice de Tom por Pa√≠s', y=1.02, fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìà S√©rie temporal do √≠ndice de tom (a partir de 2020)\n",
        "df_recent = df[df['year'] >= 2020].copy()\n",
        "\n",
        "temporal_tone = df_recent.groupby('year_month')['tone'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.lineplot(x='year_month', y='tone', data=temporal_tone, marker='o', linewidth=2)\n",
        "plt.title('üìà Varia√ß√£o do √çndice de Tom ao Longo do Tempo (2020+)', fontsize=14)\n",
        "plt.xlabel('Per√≠odo (Ano-M√™s)')\n",
        "plt.ylabel('√çndice de Tom M√©dio')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Neutro')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìà Evolu√ß√£o do √≠ndice de tom por pa√≠s (1990+)\n",
        "df_filtered = df[df['year'] >= 1990].copy()\n",
        "\n",
        "tone_by_year_country = df_filtered.groupby(['year', 'country'])['tone'].mean().unstack()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "tone_by_year_country.plot(marker='o', linewidth=2, alpha=0.8)\n",
        "plt.title('üìà Evolu√ß√£o do √çndice de Tom por Pa√≠s (1990-2023)', fontsize=14)\n",
        "plt.xlabel('Ano')\n",
        "plt.ylabel('Tone M√©dio')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Pa√≠s', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Boxplot do √≠ndice de tom por pa√≠s\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.boxplot(x='country', y='tone', data=df, palette='Set2')\n",
        "plt.title('üì¶ Distribui√ß√£o do √çndice de Tom por Pa√≠s', fontsize=14)\n",
        "plt.xlabel('Pa√≠s')\n",
        "plt.ylabel('√çndice de Tom')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclus√µes\n",
        "\n",
        "### Principais Achados:\n",
        "\n",
        "1. **Sentimentos**: Os discursos dos Bancos Centrais apresentam predominantemente\n",
        "   car√°ter **neutro** (~80%), com inclina√ß√£o positiva (~13%) maior que negativa (~6%)\n",
        "\n",
        "2. **T√≥picos**: Os t√≥picos mais relevantes para interpreta√ß√£o econ√¥mica incluem:\n",
        "   - Pol√≠tica monet√°ria e taxas de juros\n",
        "   - Infla√ß√£o e estabilidade de pre√ßos\n",
        "   - Crescimento econ√¥mico\n",
        "\n",
        "3. **√çndice de Tom**: A m√©dia geral est√° pr√≥xima de zero, indicando neutralidade,\n",
        "   com leve tend√™ncia hawkish na maioria dos pa√≠ses\n",
        "\n",
        "4. **Varia√ß√£o por Pa√≠s**: Diferentes Bancos Centrais apresentam padr√µes distintos\n",
        "   de comunica√ß√£o, refletindo suas estrat√©gias e contextos econ√¥micos\n",
        "\n",
        "### Poss√≠veis Melhorias Futuras:\n",
        "- Aplicar modelos mais avan√ßados (BERT, LSTM)\n",
        "- Expandir an√°lise para mais idiomas\n",
        "- Incorporar dados de mercado para valida√ß√£o"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nome_do_ambiente",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
